{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81b059d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m##############\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m#### EJ 1 ####\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m##############\u001b[39;00m\n\u001b[32m      8\u001b[39m sc = SparkContext.getOrCreate() \n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m data = \u001b[43msc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtextFile\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/calidad_aire_datos_meteo_mes.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[32m     12\u001b[39m header = data.first() \n\u001b[32m     15\u001b[39m rows = data.filter(\u001b[38;5;28;01mlambda\u001b[39;00m line: line != header) \\\n\u001b[32m     16\u001b[39m   .map(\u001b[38;5;28;01mlambda\u001b[39;00m line: line.split(\u001b[33m\"\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m)) \n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/pyspark/core/context.py:1038\u001b[39m, in \u001b[36mSparkContext.textFile\u001b[39m\u001b[34m(self, name, minPartitions, use_unicode)\u001b[39m\n\u001b[32m    977\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtextFile\u001b[39m(\n\u001b[32m    978\u001b[39m     \u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m, minPartitions: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m, use_unicode: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    979\u001b[39m ) -> RDD[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    980\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    981\u001b[39m \u001b[33;03m    Read a text file from HDFS, a local file system (available on all\u001b[39;00m\n\u001b[32m    982\u001b[39m \u001b[33;03m    nodes), or any Hadoop-supported file system URI, and return it as an\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1036\u001b[39m \u001b[33;03m    ['aa', 'bb', 'cc', 'x', 'y', 'z']\u001b[39;00m\n\u001b[32m   1037\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     minPartitions = minPartitions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefaultParallelism\u001b[49m, \u001b[32m2\u001b[39m)\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m RDD(\u001b[38;5;28mself\u001b[39m._jsc.textFile(name, minPartitions), \u001b[38;5;28mself\u001b[39m, UTF8Deserializer(use_unicode))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/pyspark/core/context.py:679\u001b[39m, in \u001b[36mSparkContext.defaultParallelism\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    667\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    668\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefaultParallelism\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m    669\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    670\u001b[39m \u001b[33;03m    Default level of parallelism to use when not given by user (e.g. for reduce tasks)\u001b[39;00m\n\u001b[32m    671\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    677\u001b[39m \u001b[33;03m    True\u001b[39;00m\n\u001b[32m    678\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m679\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsc\u001b[49m\u001b[43m.\u001b[49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.defaultParallelism()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py:1361\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1354\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1361\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m return_value = get_return_value(\n\u001b[32m   1363\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py:1036\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry=\u001b[38;5;28;01mTrue\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1016\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1034\u001b[39m \u001b[33;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1038\u001b[39m         response = connection.send_command(command)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py:284\u001b[39m, in \u001b[36mJavaClient._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection.socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py:291\u001b[39m, in \u001b[36mJavaClient._create_new_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    288\u001b[39m     connection = ClientServerConnection(\n\u001b[32m    289\u001b[39m         \u001b[38;5;28mself\u001b[39m.java_parameters, \u001b[38;5;28mself\u001b[39m.python_parameters,\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.gateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_thread_connection(connection)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py:438\u001b[39m, in \u001b[36mClientServerConnection.connect_to_java_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ssl_context:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket = \u001b[38;5;28mself\u001b[39m.ssl_context.wrap_socket(\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.socket, server_hostname=\u001b[38;5;28mself\u001b[39m.java_address)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28mself\u001b[39m.socket.makefile(\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.is_connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "\n",
    "##############\n",
    "#### EJ 1 ####\n",
    "##############\n",
    "\n",
    "sc = SparkContext.getOrCreate() \n",
    "\n",
    "\n",
    "data = sc.textFile(\"data/calidad_aire_datos_meteo_mes.csv\") \n",
    "header = data.first() \n",
    "\n",
    "\n",
    "rows = data.filter(lambda line: line != header) \\\n",
    "  .map(lambda line: line.split(\";\")) \n",
    "\n",
    "\n",
    "rows_madrid = rows.filter(lambda f: f[0] == \"28\")\n",
    "\n",
    "\n",
    "rows_temp = rows_madrid.filter(lambda f: f[3] == \"83\") \n",
    "\n",
    "\n",
    "resultado_ej1 = rows_temp.filter(\n",
    "    lambda f: any(f[k] == \"V\" for k in range(9, min(57, len(f)), 2))\n",
    ").count() \n",
    "print(\"\")\n",
    "print(\"############ Ejercicio 1 ############\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"Nº de registros de temperatura con >=1 hora válida:\", resultado_ej1)\n",
    "\n",
    "##############\n",
    "#### EJ 2 ####\n",
    "##############\n",
    "\n",
    "\n",
    "# Partimos del RDD inicial y aplicamos los filtros base de provincia y magnitud\n",
    "rows_temp = rows.filter(lambda f: f[0] == \"28\" and f[3] == \"83\") # [cite: 271, 280]\n",
    "\n",
    "# Aplicación de Θ(r) y U (flatMap) seguido de M(τ_0) (reduceByKey)\n",
    "resultado_ej2 = (\n",
    "    rows_temp\n",
    "    # flatMap utiliza composición funcional pura sin bucles 'for'\n",
    "    .flatMap(\n",
    "        lambda c: list(\n",
    "            map(\n",
    "                lambda i: (\n",
    "                    f\"{c[5]}-{c[6].zfill(2)}-{c[7].zfill(2)}\", # Clave τ(r)\n",
    "                    float(c[8 + 2 * i].replace(',', '.'))      # Valor F(v_i) [cite: 278]\n",
    "                ),\n",
    "                # Filtramos el rango de 24 horas evaluando la condición de expansión\n",
    "                filter(lambda i: c[9 + 2 * i] == \"V\", range(24)) # [cite: 274]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    # reduceByKey evalúa cada par de valores que comparten la misma fecha y conserva el mayor\n",
    "    .reduceByKey(lambda a, b: a if a > b else b)\n",
    "    \n",
    "    # [cite_start]sortByKey ordena el RDD final cronológicamente (opcional pero recomendado para la salida) [cite: 147, 149]\n",
    "    .sortByKey()\n",
    ")\n",
    "\n",
    "# [cite_start]Traemos los resultados calculados al driver en forma de lista [cite: 155, 157]\n",
    "temperaturas_maximas = resultado_ej2.collect()\n",
    "\n",
    "# Formateo e impresión funcional sin usar estructuras iterativas explícitas\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"############ Ejercicio 2 ############\")\n",
    "print(\"\")\n",
    "print('\\n'.join(\n",
    "    map(lambda x: f\"Fecha: {x[0]} | Temp Máxima: {x[1]} °C\", temperaturas_maximas) # [cite: 281]\n",
    "))\n",
    "\n",
    "\n",
    "##############\n",
    "#### EJ 3 ####\n",
    "##############\n",
    "\n",
    "\n",
    "# M(r): Filtrado por provincia y magnitud pluviométrica (89)\n",
    "rows_precip = rows.filter(lambda f: f[0] == \"28\" and f[3] == \"89\") \n",
    "\n",
    "# W: Transformación integral (map) para proyectar τ(r) y (σ(r), Σ(r))\n",
    "# Sustituimos la comprensión de listas por composición funcional pura (map y filter)\n",
    "precipitaciones_diarias = rows_precip.map(\n",
    "    lambda c: (\n",
    "        f\"{c[5]}-{c[6].zfill(2)}-{c[7].zfill(2)}\", # Clave τ(r): Fecha [cite: 283]\n",
    "        (\n",
    "           c[1], # Municipio [cite: 283]\n",
    "           c[2], # Estación [cite: 283]\n",
    "            # Σ(r): Reducción sumatoria sobre el mapeo de los índices filtrados\n",
    "            sum(\n",
    "                map(\n",
    "                    lambda i: float(c[8 + 2 * i].replace(',', '.')), \n",
    "                    filter(lambda i: c[9 + 2 * i] == \"V\", range(24))\n",
    "                )\n",
    "            ) \n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# S(τ): Reducción por clave temporal para hallar el supremo local (reduceByKey)\n",
    "maximos_diarios_estacion = (\n",
    "    precipitaciones_diarias\n",
    "    .reduceByKey(lambda a, b: a if a[2] > b[2] else b) \n",
    "    .sortByKey() [cite: 147, 149]\n",
    ")\n",
    "\n",
    "# Ω: Reducción escalar global (reduce) para extraer el supremo absoluto \n",
    "# Compara pares completos (fecha, (municipio, estacion, precipitación)) y conserva el de mayor magnitud\n",
    "maximo_absoluto = maximos_diarios_estacion.reduce(\n",
    "    lambda a, b: a if a[1][2] > b[1][2] else b \n",
    ")\n",
    "\n",
    "# [cite_start]Extracción de la topología local (collect) al driver [cite: 155, 157]\n",
    "resultados_locales = maximos_diarios_estacion.collect() \n",
    "\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"############ Ejercicio 3 ############\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"--- MÁXIMOS DIARIOS POR ESTACIÓN ---\")\n",
    "# Impresión funcional acoplada prescindiendo del bucle for\n",
    "print('\\n'.join(\n",
    "    map(\n",
    "        lambda r: f\"Fecha: {r[0]} | Municipio: {r[1][0]} | Estación: {r[1][1]} | Precipitación Total: {r[1][2]}\", \n",
    "        resultados_locales\n",
    "    )\n",
    "[cite_start])) \n",
    "\n",
    "print(\"\\n--- MÁXIMO ABSOLUTO DEL PERIODO ---\")\n",
    "print(f\"Fecha: {maximo_absoluto[0]} | Municipio: {maximo_absoluto[1][0]} | Estación: {maximo_absoluto[1][1]} | Precipitación: {maximo_absoluto[1][2]}\") \n",
    "\n",
    "\n",
    "##############\n",
    "#### EJ 4 ####\n",
    "##############\n",
    "\n",
    "\n",
    "rows_temp = rows.filter(lambda f: f[0] == \"28\" and f[3] == \"83\")\n",
    "\n",
    "\n",
    "estaciones_medias = (\n",
    "    rows_temp.map(\n",
    "        lambda c: (\n",
    "            c[1], \n",
    "            c[2], \n",
    "            f\"{c[5]}-{c[6].zfill(2)}-{c[7].zfill(2)}\", \n",
    "\n",
    "            [float(c[8 + 2 * i].replace(',', '.')) for i in range(24) if c[9 + 2 * i] == \"V\"] \n",
    "        )\n",
    "    )\n",
    "    .filter(lambda x: len(x[3]) > 0) \n",
    "    .map(lambda x: (x[0], x[1], x[2], sum(x[3]) / len(x[3]))) \n",
    ")\n",
    "\n",
    "\n",
    "estacion_A = (\n",
    "    estaciones_medias\n",
    "    .filter(lambda x: x[0] == \"6\" and x[1] == \"4\")\n",
    "    .map(lambda x: (x[2], x[3]))\n",
    ")\n",
    "\n",
    "\n",
    "estacion_B = (\n",
    "    estaciones_medias\n",
    "    .filter(lambda x: x[0] == \"5\" and x[1] == \"2\") \n",
    "    .map(lambda x: (x[2], x[3])) \n",
    ")\n",
    "\n",
    "\n",
    "comparacion_porcentual = (\n",
    "    estacion_A.join(estacion_B) \n",
    "    .map(lambda j: (j[0], (j[1][1] / j[1][0]) * 100)) \n",
    "    .sortByKey() \n",
    ")\n",
    "\n",
    "\n",
    "resultados_ej4 = comparacion_porcentual.collect() \n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"############ Ejercicio 4 ############\")\n",
    "print(\"\")\n",
    "print(\"--- COMPARACIÓN PORCENTUAL DE TEMPERATURAS MEDIAS ---\")\n",
    "for fecha, porcentaje in resultados_ej4:\n",
    "    print(f\"Fecha: {fecha} | Porcentaje (Est. 5-2 vs Ref 6-4): {porcentaje:.2f}%\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
